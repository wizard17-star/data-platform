# Mini Data Platform - End-to-End CDC & Delta Lake

A complete data engineering project demonstrating modern data pipeline architecture using **PostgreSQL** â†’ **Debezium** â†’ **Kafka** â†’ **Spark** â†’ **Delta Lake** (MinIO) stack.

## ğŸ¯ Project Overview

This platform implements the **Medallion Architecture** pattern:

- **Bronze Layer**: Raw data ingestion from Kafka
- **Silver Layer**: Data cleaning and transformation  
- **Gold Layer**: Dimensional modeling for analytics

### Tech Stack
- PostgreSQL (source database)
- Debezium (Change Data Capture)
- Apache Kafka (streaming)
- Apache Spark (processing)
- Delta Lake + MinIO (storage)
- Docker (containerization)

---

## ğŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose
- 4GB+ RAM
- Git Bash (Windows users)

### Option 1: Automated Setup (Recommended)
```bash
bash deploy.sh
```

### Option 2: Manual Setup

**1. Start all services**
```bash
docker-compose up -d
```

**2. Load sample data**
```bash
docker exec -it loader python /app/load_to_postgres.py
```

**3. Register Debezium CDC connector**
```bash
curl -X POST -H "Content-Type:application/json" localhost:8083/connectors/ -d '{
  "name": "pg-connector",
  "config": {
    "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
    "database.hostname": "pg",
    "database.port": "5432",
    "database.user": "app",
    "database.password": "app",
    "database.dbname": "appdb",
    "topic.prefix": "dbserver1",
    "plugin.name": "pgoutput",
    "key.converter": "org.apache.kafka.connect.json.JsonConverter",
    "value.converter": "org.apache.kafka.connect.json.JsonConverter",
    "key.converter.schemas.enable": "false",
    "value.converter.schemas.enable": "false"
  }
}'
```

**4. Start Spark streaming job**
```bash
docker-compose run --rm spark spark-submit \
  --packages io.delta:delta-core_2.12:2.4.0 \
  /opt/jobs/process.py
```

---

## ğŸ“Š Verify the Pipeline

### Check Kafka Topics
```bash
docker exec kafka kafka-topics --bootstrap-server kafka:29092 --list
docker exec kafka kafka-console-consumer --bootstrap-server kafka:29092 \
  --topic dbserver1.public.customer --from-beginning
```

### View Delta Lake Data
**MinIO UI**: http://localhost:9001 (minio / minio12345)

Folders:
- `/bronze/all_events` - raw messages
- `/silver/customer`, `/silver/product`, `/silver/salesorder` - cleaned data
- `/gold/dim_customer`, `/gold/dim_product`, `/gold/fact_sales` - analytical tables
## ğŸ“ Project Structure

```
mini-platform/
â”œâ”€â”€ docker-compose.yml          # Services orchestration
â”œâ”€â”€ deploy.sh                   # One-command deployment
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ load_to_postgres.py     # Data loader
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ spark_jobs/
â”‚   â”œâ”€â”€ process.py              # Spark streaming job
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ connect-docker/             # Debezium connector
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ customer.csv
â”‚   â”œâ”€â”€ product.csv
â”‚   â””â”€â”€ salesorder.csv
â””â”€â”€ README.md
```

---

## ğŸ”§ Troubleshooting

| Issue | Solution |
|-------|----------|
| Containers won't start | `docker-compose logs <service>` |
| Kafka topics not found | Check connector's `topic.prefix` matches Spark job |
| MinIO bucket missing | `docker exec minio mc mb local/lake` |
| Out of memory | Increase Docker RAM (4GB minimum) |

---

## ğŸ“ How It Works

1. **CSV Data** â†’ PostgreSQL tables
2. **Database Changes** â†’ Debezium detects (INSERT/UPDATE/DELETE)
3. **Kafka Topics** â†’ Changes streamed as JSON messages
4. **Spark Jobs** â†’ Consume & transform (Bronze â†’ Silver â†’ Gold)
5. **Delta Lake** â†’ ACID-compliant data warehouse

---

## ğŸ“ Learning Outcomes

- Real-time CDC architecture patterns
- Streaming data processing with Spark
- Delta Lake for data warehousing
- Docker orchestration
- Medallion Architecture design

---

## ğŸ“š Key Files

- `docker-compose.yml` - Full stack configuration
- `spark_jobs/process.py` - Spark job with layered processing
- `app/load_to_postgres.py` - Sample data loader
- `deploy.sh` - Automated setup script

---

**Created**: January 2026  
**License**: Open Source